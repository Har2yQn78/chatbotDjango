{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4ed78f7-5dff-46ac-8651-98e35e4471c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import setup\n",
    "import os\n",
    "import numpy as np\n",
    "from decouple import config, AutoConfig\n",
    "config = AutoConfig(search_path=\"/home/harry/chatbotDjango\") \n",
    "\n",
    "setup.init_django()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c86d61c-c2d6-48dc-9591-51d8f155305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.models import BlogPost, EMEDDING_LENGTH\n",
    "from data import services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5221489a-41e1-4341-9392-1ef6f203f988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<QuerySet [<BlogPost: BlogPost object (25)>, <BlogPost: BlogPost object (26)>, <BlogPost: BlogPost object (27)>, <BlogPost: BlogPost object (28)>]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qs = BlogPost.objects.filter(can_delete=True)\n",
    "# qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb8b8dbf-928a-4446-b340-a6e2828f3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fe2d406-c30a-4481-99d9-767b4923be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "MISTRAL_API_KEY = config(\"MISTRAL_API_KEY\")\n",
    "\n",
    "EMEDDING_LENGTH=config(\"EMEDDING_LENGTH\", default=1024, cast=int)\n",
    "\n",
    "LLM_CONFIG = {\n",
    "    \"api_key\" : MISTRAL_API_KEY,\n",
    "}\n",
    "\n",
    "EMBED_CONFIG = {\n",
    "    \"api_key\" : MISTRAL_API_KEY,\n",
    "}\n",
    "\n",
    "llm = Mistral(**LLM_CONFIG)\n",
    "embed_model = Mistral(**EMBED_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "394bcf4c-d57a-413c-9ca4-a73c61cf1834",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Settings\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mSettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m = llm\n\u001b[32m      4\u001b[39m Settings.embed_model = embed_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/chatbotDjango/venv/lib/python3.11/site-packages/llama_index/core/settings.py:46\u001b[39m, in \u001b[36m_Settings.llm\u001b[39m\u001b[34m(self, llm)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;129m@llm\u001b[39m.setter\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm: LLMType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     45\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Set the LLM.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28mself\u001b[39m._llm = \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/chatbotDjango/venv/lib/python3.11/site-packages/llama_index/core/llms/utils.py:102\u001b[39m, in \u001b[36mresolve_llm\u001b[39m\u001b[34m(llm, callback_manager)\u001b[39m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLLM is explicitly disabled. Using MockLLM.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    100\u001b[39m     llm = MockLLM()\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, LLM)\n\u001b[32m    104\u001b[39m llm.callback_manager = callback_manager \u001b[38;5;129;01mor\u001b[39;00m Settings.callback_manager\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba60056-2d72-4d9d-9430-a78a0506ecce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
