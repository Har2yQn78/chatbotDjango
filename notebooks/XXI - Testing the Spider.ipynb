{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be03cf7-e03e-48b7-ae97-a0638fcf296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Union, List, Dict, Any\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from sqlalchemy import create_engine, text, MetaData, Table, Column\n",
    "\n",
    "from llama_index.core import SQLDatabase\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "\n",
    "import setup\n",
    "setup.init_django()\n",
    "\n",
    "from rag import (\n",
    "    db as rag_db,\n",
    "    engines as rag_engines,\n",
    "    settings as rag_settings,\n",
    "    prompts as rag_prompts,\n",
    "    patches as rag_patches,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60338609-9938-445b-afb7-692d87ea1c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderDatabaseManager:\n",
    "    def __init__(self, spider_dir=\"/home/harry/chatbotDjango/spider/spider_data\"):\n",
    "        self.spider_dir = spider_dir\n",
    "        self.db_dir = os.path.join(spider_dir, \"database\")\n",
    "        self.tables_file = os.path.join(spider_dir, \"tables.json\")\n",
    "        self.db_schemas = self._load_db_schemas()\n",
    "        \n",
    "    def _load_db_schemas(self):\n",
    "        \"\"\"Load schema information for all Spider databases\"\"\"\n",
    "        with open(self.tables_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "    def get_db_path(self, db_id):\n",
    "        \"\"\"Get SQLite file path for a database ID\"\"\"\n",
    "        return os.path.join(self.db_dir, db_id, f\"{db_id}.sqlite\")\n",
    "    \n",
    "    def get_schema_for_db(self, db_id):\n",
    "        \"\"\"Get schema information for specific database\"\"\"\n",
    "        for db_schema in self.db_schemas:\n",
    "            if db_schema['db_id'] == db_id:\n",
    "                return db_schema\n",
    "        return None\n",
    "    \n",
    "    def get_all_db_ids(self):\n",
    "        \"\"\"Get all database IDs in Spider benchmark\"\"\"\n",
    "        return [db_schema['db_id'] for db_schema in self.db_schemas]\n",
    "    \n",
    "    def get_sqlalchemy_engine(self, db_id):\n",
    "        \"\"\"Create SQLAlchemy engine for a database\"\"\"\n",
    "        db_path = self.get_db_path(db_id)\n",
    "        return create_engine(f\"sqlite:///{db_path}\")\n",
    "    \n",
    "    def get_table_names(self, db_id):\n",
    "        \"\"\"Get all table names for a database\"\"\"\n",
    "        schema = self.get_schema_for_db(db_id)\n",
    "        if schema:\n",
    "            if isinstance(schema['table_names_original'], list):\n",
    "                # Check if the list contains strings or something else\n",
    "                if all(isinstance(item, str) for item in schema['table_names_original']):\n",
    "                    return schema['table_names_original']\n",
    "                else:\n",
    "                    # If table_names_original contains objects, try to extract table_name\n",
    "                    return [table['table_name'] if isinstance(table, dict) and 'table_name' in table \n",
    "                           else str(table) for table in schema['table_names_original']]\n",
    "        return []\n",
    "    \n",
    "    def get_llama_index_database(self, db_id):\n",
    "        \"\"\"Create LlamaIndex SQLDatabase for Spider database\"\"\"\n",
    "        engine = self.get_sqlalchemy_engine(db_id)\n",
    "        table_names = self.get_table_names(db_id)\n",
    "        # NEW: Verify tables\n",
    "        print(f\"Final tables for {db_id}: {table_names}\")  # NEW\n",
    "        return SQLDatabase(engine, include_tables=table_names)\n",
    "\n",
    "spider_manager = SpiderDatabaseManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "958fa294-e2f9-432e-8b3f-5488f1b1fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded Spider dev data with 1034 entries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# 1. Define the correct path using raw string format\n",
    "SPIDER_DIR = \"/home/harry/chatbotDjango/spider/spider_data\"\n",
    "\n",
    "# 2. Verify the path exists\n",
    "if not os.path.exists(SPIDER_DIR):\n",
    "    raise FileNotFoundError(f\"Spider directory not found at: {SPIDER_DIR}\")\n",
    "\n",
    "# 3. Verify dev.json exists\n",
    "DEV_JSON_PATH = os.path.join(SPIDER_DIR, \"dev.json\")\n",
    "if not os.path.exists(DEV_JSON_PATH):\n",
    "    raise FileNotFoundError(f\"dev.json not found at: {DEV_JSON_PATH}\")\n",
    "\n",
    "def load_spider_data(split=\"dev\"):\n",
    "    \"\"\"Load Spider evaluation data for specified split\"\"\"\n",
    "    file_path = os.path.join(SPIDER_DIR, f\"{split}.json\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except UnicodeDecodeError as e:\n",
    "        raise ValueError(f\"Error decoding JSON file: {e}\") from e\n",
    "\n",
    "# 4. Load the data with verification\n",
    "try:\n",
    "    spider_dev = load_spider_data(\"dev\")\n",
    "    print(f\"Successfully loaded Spider dev data with {len(spider_dev)} entries\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d067c19-d772-49c5-8846-fe39d9f6ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spider_sql_query_engine(db_id):\n",
    "    \"\"\"Create SQL query engine for Spider database\"\"\"\n",
    "    sql_database = spider_manager.get_llama_index_database(db_id)\n",
    "    table_names = spider_manager.get_table_names(db_id)\n",
    "    \n",
    "    text_to_sql_prompt = rag_prompts.custom_text_to_sql_prompt\n",
    "    text_to_sql_prompt.template = text_to_sql_prompt.template.replace(\n",
    "        \"{dialect} PostgreSQL\", \"SQLite\"\n",
    "    )\n",
    "    \n",
    "    sql_query_engine = NLSQLTableQueryEngine(\n",
    "        sql_database=sql_database,\n",
    "        tables=table_names,\n",
    "        response_synthesis_prompt=rag_prompts.custom_sql_response_synthesis_prompt,\n",
    "        text_to_sql_prompt=text_to_sql_prompt\n",
    "    )\n",
    "    \n",
    "    # Use the corrected import for QueryEngineTool\n",
    "    from llama_index.core.tools import QueryEngineTool\n",
    "    \n",
    "    sql_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=sql_query_engine,\n",
    "        description=f\"Useful for translating natural language queries into SQL over {db_id} database\"\n",
    "    )\n",
    "    \n",
    "    return sql_query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b5117c-b796-48a2-b5e4-25f433c776e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Global rate limiting state\n",
    "LAST_API_CALL_TIME = 0\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def api_call_with_retry(func):\n",
    "    \"\"\"Decorator to handle rate limiting and retries for any API call\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        global LAST_API_CALL_TIME, MAX_RETRIES\n",
    "        retries = 0\n",
    "        \n",
    "        while retries <= MAX_RETRIES:\n",
    "            # Enforce rate limiting\n",
    "            elapsed = time.time() - LAST_API_CALL_TIME\n",
    "            if elapsed < 5:\n",
    "                sleep_time = 5 - elapsed\n",
    "                print(f\"Global rate limiting: Sleeping {sleep_time:.1f}s\")\n",
    "                time.sleep(sleep_time)\n",
    "            \n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                LAST_API_CALL_TIME = time.time()\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                if \"rate limit\" in str(e).lower():\n",
    "                    print(f\"Rate limit hit ({func.__name__}). Waiting 5s (retry {retries+1}/{MAX_RETRIES})\")\n",
    "                    time.sleep(5)\n",
    "                    LAST_API_CALL_TIME = time.time()\n",
    "                    retries += 1\n",
    "                else:\n",
    "                    print(f\"Non-retryable error in {func.__name__}: {str(e)}\")\n",
    "                    raise\n",
    "        \n",
    "        print(f\"Max retries exceeded for {func.__name__}. Raising error.\")\n",
    "        raise RuntimeError(f\"API call failed after {MAX_RETRIES} retries\")\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a758dde6-6587-4a0d-91ed-9987fc81b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "@api_call_with_retry\n",
    "def translate_to_farsi(text):\n",
    "    \"\"\"Translate English to Farsi using LLM\"\"\"\n",
    "    prompt = f\"Translate English to Farsi. Only return translation:\\n{text}\"\n",
    "    response = rag_settings.Settings.llm.complete(prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbdba1df-a3aa-4870-a078-bb83562beb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "@api_call_with_retry\n",
    "def execute_query(query_engine, query_text):\n",
    "    \"\"\"Execute a query with rate limiting\"\"\"\n",
    "    return query_engine.query(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcb57acc-ba34-4fe2-ad04-c3beffb4d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_spider_schema(db_id):\n",
    "    schema = spider_manager.get_schema_for_db(db_id)\n",
    "    if not schema:\n",
    "        print(f\"No schema found for database: {db_id}\")\n",
    "        return\n",
    "    print(f\"Database ID: {db_id}\")\n",
    "    print(f\"Schema keys: {list(schema.keys())}\")\n",
    "    print(\"\\nTable Names Original structure:\")\n",
    "    if 'table_names_original' in schema:\n",
    "        table_names = schema['table_names_original']\n",
    "        print(f\"Type: {type(table_names)}\")\n",
    "        print(f\"Length: {len(table_names)}\")\n",
    "        if len(table_names) > 0:\n",
    "            print(f\"First item type: {type(table_names[0])}\")\n",
    "            print(f\"First item: {table_names[0]}\")\n",
    "    else:\n",
    "        print(\"No table_names_original in schema\")\n",
    "    all_db_ids = spider_manager.get_all_db_ids()\n",
    "    if all_db_ids:\n",
    "        print(f\"\\nAll database IDs (first 5): {all_db_ids[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c3ec60-7bbb-4edb-b265-1d37bbd44fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def extract_sql_from_response(response):\n",
    "    if response is None:\n",
    "        return \"N/A (No Response)\"\n",
    "    if hasattr(response, 'metadata') and getattr(response, 'metadata', None) and 'sql_query' in response.metadata:\n",
    "        return response.metadata['sql_query']\n",
    "    if hasattr(response, 'source_nodes'):\n",
    "        for node in getattr(response, 'source_nodes', []):\n",
    "            node_metadata = getattr(node, 'metadata', {})\n",
    "            if 'sql_query' in node_metadata:\n",
    "                return node_metadata['sql_query']\n",
    "    response_text = str(getattr(response, 'response', str(response)))\n",
    "    patterns = [\n",
    "        r\"```sql\\s*([\\s\\S]*?)\\s*```\",\n",
    "        r\"SQL: (.*?);\",\n",
    "        r\"SELECT.*?FROM.*?(?:WHERE.*?|ORDER BY.*?|LIMIT.*?)?$\"\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response_text, re.IGNORECASE | re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    return response_text\n",
    "\n",
    "def evaluate_on_spider_sample(sample_size=3, use_farsi=False, random_seed=42):\n",
    "    import time\n",
    "    np.random.seed(random_seed)\n",
    "    results = []\n",
    "    db_counts = {}\n",
    "    for ex in spider_dev:\n",
    "        db_counts[ex['db_id']] = db_counts.get(ex['db_id'], 0) + 1\n",
    "    valid_dbs = [db_id for db_id, count in db_counts.items() if count > 0]\n",
    "    if not valid_dbs:\n",
    "        print(\"No valid databases found with examples\")\n",
    "        return results\n",
    "    selected_dbs = np.random.choice(valid_dbs, min(sample_size, len(valid_dbs)), replace=False)\n",
    "    for db_id in selected_dbs:\n",
    "        print(f\"\\n{'='*40}\\nProcessing database: {db_id}\")\n",
    "        translation_cache = {} if use_farsi else None\n",
    "        db_examples = [ex for ex in spider_dev if ex['db_id'] == db_id]\n",
    "        if len(db_examples) == 0:\n",
    "            print(f\"Skipping {db_id} - no examples available\")\n",
    "            continue\n",
    "        actual_sample_size = min(2, len(db_examples))\n",
    "        try:\n",
    "            if actual_sample_size == 1:\n",
    "                sample_examples = db_examples\n",
    "            else:\n",
    "                sample_examples = np.random.choice(db_examples, actual_sample_size, replace=False)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error sampling examples for {db_id}: {str(e)}\")\n",
    "            continue\n",
    "        try:\n",
    "            query_engine = create_spider_sql_query_engine(db_id)\n",
    "            test_response = execute_query(query_engine, \"SELECT 1\")\n",
    "            if not test_response:\n",
    "                raise ValueError(\"Query engine failed basic connectivity test\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create engine for {db_id}: {str(e)}\")\n",
    "            continue\n",
    "        for example in tqdm(sample_examples, desc=f\"Evaluating {db_id}\"):\n",
    "            question = example['question']\n",
    "            query_text = None \n",
    "            try:\n",
    "                if use_farsi:\n",
    "                    if question not in translation_cache:\n",
    "                        translation_cache[question] = translate_to_farsi(question)\n",
    "                    query_text = translation_cache[question]\n",
    "                else:\n",
    "                    query_text = question\n",
    "                response = execute_query(query_engine, query_text)\n",
    "                predicted_sql = extract_sql_from_response(response)\n",
    "                if not re.search(r\"(SELECT|INSERT|UPDATE|DELETE)\", predicted_sql, re.IGNORECASE):\n",
    "                    raise ValueError(\"Generated SQL appears invalid\")\n",
    "                results.append({\n",
    "                    'db_id': db_id,\n",
    "                    'question': question,\n",
    "                    'farsi_question': query_text if use_farsi else None,\n",
    "                    'predicted_sql': predicted_sql,\n",
    "                    'gold_sql': example['query'],\n",
    "                    'success': True\n",
    "                })\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                print(f\"\\nError processing query: {traceback.format_exc()}\")\n",
    "                results.append({\n",
    "                    'db_id': db_id,\n",
    "                    'question': question,\n",
    "                    'farsi_question': query_text if use_farsi else None,  # \n",
    "                    'gold_sql': example.get('query', 'Not available'),\n",
    "                    'error': str(e),\n",
    "                    'success': False\n",
    "                })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e2ce6-8241-4425-ba79-8c6213228e97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing database: concert_singer\n",
      "Final tables for concert_singer: ['stadium', 'singer', 'concert', 'singer_in_concert']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c87ded0f4f45409cb0e52b1f897216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating concert_singer:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global rate limiting: Sleeping 5.0s\n",
      "Global rate limiting: Sleeping 5.0s\n",
      "\n",
      "========================================\n",
      "Processing database: dog_kennels\n",
      "Final tables for dog_kennels: ['Breeds', 'Charges', 'Sizes', 'Treatment_Types', 'Owners', 'Dogs', 'Professionals', 'Treatments']\n",
      "Global rate limiting: Sleeping 5.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2492363a034cf9b61d6fde0351274f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating dog_kennels:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global rate limiting: Sleeping 5.0s\n"
     ]
    }
   ],
   "source": [
    "# English evaluation\n",
    "english_results = evaluate_on_spider_sample(sample_size=2, use_farsi=False)\n",
    "\n",
    "# Farsi evaluation \n",
    "farsi_results = evaluate_on_spider_sample(sample_size=2, use_farsi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f98b9f-962f-454d-83a8-6473362f0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(results)\n",
    "    if df.empty:\n",
    "        print(\"No results available\")\n",
    "        return df\n",
    "    if 'success' not in df.columns:\n",
    "        print(\"No successful queries found\")\n",
    "        df['success'] = False\n",
    "    success_rate = df['success'].mean() if 'success' in df.columns else 0\n",
    "    print(f\"Total Queries: {len(df)}\")\n",
    "    print(f\"Success Rate: {success_rate:.2%}\")\n",
    "    if 'error' in df.columns and (~df['success']).any():\n",
    "        print(\"\\nError Distribution:\")\n",
    "        print(df[~df['success']]['error'].value_counts())\n",
    "    if 'db_id' in df.columns and not df.empty:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        df['db_id'].value_counts().plot(kind='bar')\n",
    "        plt.title(\"Query Distribution by Database\")\n",
    "        plt.show()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468e92c-0efd-4206-bd93-c24c82925cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the existing data frame creation issue\n",
    "english_df = analyze_results(english_results)\n",
    "farsi_df = analyze_results(farsi_results)\n",
    "\n",
    "# Now create the comparison bar chart correctly\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(['English', 'Farsi'], \n",
    "        [english_df['success'].mean() if not english_df.empty and 'success' in english_df.columns else 0, \n",
    "         farsi_df['success'].mean() if not farsi_df.empty and 'success' in farsi_df.columns else 0])\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Success Rate Comparison\")\n",
    "plt.ylabel(\"Success Rate\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Create a more detailed comparison function\n",
    "def create_detailed_evaluation_report(english_results, farsi_results):\n",
    "    \"\"\"Generate detailed comparison between English and Farsi SQL generation\"\"\"\n",
    "    # Create a combined DataFrame with language indicator\n",
    "    if not english_results and not farsi_results:\n",
    "        print(\"No results available for analysis\")\n",
    "        return\n",
    "    \n",
    "    # Add language indicator to each result set\n",
    "    for r in english_results:\n",
    "        r['language'] = 'English'\n",
    "    for r in farsi_results:\n",
    "        r['language'] = 'Farsi'\n",
    "    \n",
    "    # Combine results\n",
    "    all_results = english_results + farsi_results\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # 1. Print overall statistics\n",
    "    print(f\"Total queries evaluated: {len(df)}\")\n",
    "    print(f\"English queries: {len(english_results)}\")\n",
    "    print(f\"Farsi queries: {len(farsi_results)}\")\n",
    "    \n",
    "    # 2. Success rates by language\n",
    "    success_by_lang = df.groupby('language')['success'].agg(['count', 'mean'])\n",
    "    success_by_lang.columns = ['Total Queries', 'Success Rate']\n",
    "    success_by_lang['Success Rate'] = success_by_lang['Success Rate'].apply(lambda x: f\"{x:.2%}\")\n",
    "    print(\"\\nSuccess Rate by Language:\")\n",
    "    print(success_by_lang)\n",
    "    \n",
    "    # 3. Paired comparison for same questions\n",
    "    if english_results and farsi_results:\n",
    "        # Create lookup dictionaries for both result sets\n",
    "        english_dict = {r['question']: r for r in english_results}\n",
    "        farsi_dict = {r['question']: r for r in farsi_results}\n",
    "        \n",
    "        # Find common questions\n",
    "        common_questions = set(english_dict.keys()) & set(farsi_dict.keys())\n",
    "        \n",
    "        if common_questions:\n",
    "            print(f\"\\nFound {len(common_questions)} queries that were tested in both languages\")\n",
    "            \n",
    "            # Create paired comparison table\n",
    "            paired_rows = []\n",
    "            for q in common_questions:\n",
    "                en_result = english_dict[q]\n",
    "                fa_result = farsi_dict[q]\n",
    "                \n",
    "                paired_rows.append({\n",
    "                    'Question': q,\n",
    "                    'English_Success': en_result.get('success', False),\n",
    "                    'Farsi_Success': fa_result.get('success', False),\n",
    "                    'English_SQL': en_result.get('predicted_sql', 'N/A'),\n",
    "                    'Farsi_SQL': fa_result.get('predicted_sql', 'N/A'),\n",
    "                    'Gold_SQL': en_result.get('gold_sql', 'N/A'),\n",
    "                    'Farsi_Translation': fa_result.get('farsi_question', 'N/A')\n",
    "                })\n",
    "            \n",
    "            paired_df = pd.DataFrame(paired_rows)\n",
    "            \n",
    "            # Calculate agreement statistics\n",
    "            both_success = sum((paired_df['English_Success'] & paired_df['Farsi_Success']))\n",
    "            both_fail = sum((~paired_df['English_Success'] & ~paired_df['Farsi_Success']))\n",
    "            en_only = sum((paired_df['English_Success'] & ~paired_df['Farsi_Success']))\n",
    "            fa_only = sum((~paired_df['English_Success'] & paired_df['Farsi_Success']))\n",
    "            \n",
    "            print(f\"Both languages succeeded: {both_success} ({both_success/len(paired_df):.2%})\")\n",
    "            print(f\"Both languages failed: {both_fail} ({both_fail/len(paired_df):.2%})\")\n",
    "            print(f\"Only English succeeded: {en_only} ({en_only/len(paired_df):.2%})\")\n",
    "            print(f\"Only Farsi succeeded: {fa_only} ({fa_only/len(paired_df):.2%})\")\n",
    "            \n",
    "            # Display example translations and SQL generations\n",
    "            if len(paired_df) > 0:\n",
    "                print(\"\\nExample Translation and SQL Generation:\")\n",
    "                for idx, row in paired_df.head(min(3, len(paired_df))).iterrows():\n",
    "                    print(f\"\\nQuestion: {row['Question']}\")\n",
    "                    print(f\"Farsi Translation: {row['Farsi_Translation']}\")\n",
    "                    print(f\"Gold SQL: {row['Gold_SQL']}\")\n",
    "                    print(f\"English Generated SQL: {row['English_SQL']}\")\n",
    "                    print(f\"Farsi Generated SQL: {row['Farsi_SQL']}\")\n",
    "                    print(f\"Match: {'✓' if row['English_Success'] and row['Farsi_Success'] else '✗'}\")\n",
    "        else:\n",
    "            print(\"\\nNo common questions found between English and Farsi evaluations\")\n",
    "    \n",
    "    # 4. Error analysis\n",
    "    if 'error' in df.columns:\n",
    "        print(\"\\nError Analysis:\")\n",
    "        error_by_lang = df[~df['success']].groupby(['language', 'error']).size().reset_index()\n",
    "        error_by_lang.columns = ['Language', 'Error Type', 'Count']\n",
    "        error_by_lang = error_by_lang.sort_values(['Language', 'Count'], ascending=[True, False])\n",
    "        \n",
    "        # Group similar errors\n",
    "        error_by_lang['Error Category'] = error_by_lang['Error Type'].apply(categorize_error)\n",
    "        \n",
    "        error_categories = error_by_lang.groupby(['Language', 'Error Category']).agg({'Count': 'sum'}).reset_index()\n",
    "        error_categories = error_categories.sort_values(['Language', 'Count'], ascending=[True, False])\n",
    "        \n",
    "        # Plot error distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for i, lang in enumerate(['English', 'Farsi']):\n",
    "            lang_errors = error_categories[error_categories['Language'] == lang]\n",
    "            if not lang_errors.empty:\n",
    "                plt.subplot(1, 2, i+1)\n",
    "                plt.pie(lang_errors['Count'], labels=lang_errors['Error Category'], \n",
    "                        autopct='%1.1f%%', startangle=90)\n",
    "                plt.axis('equal')\n",
    "                plt.title(f'{lang} Error Categories')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 5. Database-wise performance\n",
    "    if 'db_id' in df.columns:\n",
    "        print(\"\\nPerformance by Database:\")\n",
    "        db_performance = df.groupby(['language', 'db_id'])['success'].agg(['count', 'mean'])\n",
    "        db_performance.columns = ['Total Queries', 'Success Rate']\n",
    "        print(db_performance)\n",
    "        \n",
    "        # Plot database performance comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        db_success = df.pivot_table(index='db_id', columns='language', \n",
    "                                   values='success', aggfunc='mean')\n",
    "        db_success.plot(kind='bar', figsize=(12, 6))\n",
    "        plt.title('Success Rate by Database and Language')\n",
    "        plt.ylabel('Success Rate')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def categorize_error(error_text):\n",
    "    \"\"\"Categorize error messages into broader categories\"\"\"\n",
    "    error_text = str(error_text).lower()\n",
    "    \n",
    "    if 'sql' in error_text and ('invalid' in error_text or 'syntax' in error_text):\n",
    "        return 'SQL Syntax Error'\n",
    "    elif 'generated sql appears invalid' in error_text:\n",
    "        return 'Invalid SQL Generation'\n",
    "    elif 'connection' in error_text or 'timeout' in error_text:\n",
    "        return 'Connection/Timeout Error'\n",
    "    elif 'schema' in error_text or 'table' in error_text or 'column' in error_text:\n",
    "        return 'Schema Understanding Error'\n",
    "    elif 'query' in error_text and 'execution' in error_text:\n",
    "        return 'Query Execution Error'\n",
    "    elif 'translation' in error_text:\n",
    "        return 'Translation Error'\n",
    "    else:\n",
    "        return 'Other Error'\n",
    "\n",
    "# Run the detailed analysis\n",
    "combined_df = create_detailed_evaluation_report(english_results, farsi_results)\n",
    "\n",
    "# Add visualization of translation quality\n",
    "if farsi_results:\n",
    "    # Extract sample translations\n",
    "    translations = [(r['question'], r['farsi_question']) for r in farsi_results if 'farsi_question' in r and r['farsi_question']]\n",
    "    \n",
    "    if translations:\n",
    "        # Display sample translations\n",
    "        print(\"\\nSample English to Farsi Translations:\")\n",
    "        for i, (en, fa) in enumerate(translations[:5]):\n",
    "            print(f\"{i+1}. English: {en}\")\n",
    "            print(f\"   Farsi:   {fa}\")\n",
    "            print()\n",
    "        \n",
    "        # Create translation analysis\n",
    "        translation_success = sum(1 for r in farsi_results if r.get('success', False))\n",
    "        print(f\"Total translation attempts: {len(translations)}\")\n",
    "        print(f\"Successful queries after translation: {translation_success} ({translation_success/len(translations):.2%})\")\n",
    "\n",
    "# Compare SQL generation complexity\n",
    "if english_results and farsi_results:\n",
    "    print(\"\\nSQL Complexity Analysis:\")\n",
    "    \n",
    "    def analyze_sql_complexity(sql):\n",
    "        \"\"\"Calculate complexity metrics for SQL query\"\"\"\n",
    "        if not sql or not isinstance(sql, str):\n",
    "            return {'length': 0, 'joins': 0, 'conditions': 0, 'aggregations': 0}\n",
    "        \n",
    "        # Count SQL features\n",
    "        joins = len(re.findall(r'\\bjoin\\b', sql, re.IGNORECASE))\n",
    "        conditions = len(re.findall(r'\\bwhere\\b|\\band\\b|\\bor\\b', sql, re.IGNORECASE))\n",
    "        aggregations = len(re.findall(r'\\bcount\\b|\\bsum\\b|\\bavg\\b|\\bmax\\b|\\bmin\\b', sql, re.IGNORECASE))\n",
    "        \n",
    "        return {\n",
    "            'length': len(sql),\n",
    "            'joins': joins,\n",
    "            'conditions': conditions,\n",
    "            'aggregations': aggregations\n",
    "        }\n",
    "    \n",
    "    # Analyze only successful generations\n",
    "    en_successful = [r for r in english_results if r.get('success', False) and 'predicted_sql' in r]\n",
    "    fa_successful = [r for r in farsi_results if r.get('success', False) and 'predicted_sql' in r]\n",
    "    \n",
    "    en_complexity = [analyze_sql_complexity(r['predicted_sql']) for r in en_successful]\n",
    "    fa_complexity = [analyze_sql_complexity(r['predicted_sql']) for r in fa_successful]\n",
    "    \n",
    "    if en_complexity and fa_complexity:\n",
    "        # Calculate averages\n",
    "        en_avg = {k: sum(d[k] for d in en_complexity) / len(en_complexity) for k in en_complexity[0]}\n",
    "        fa_avg = {k: sum(d[k] for d in fa_complexity) / len(fa_complexity) for k in fa_complexity[0]}\n",
    "        \n",
    "        # Print complexity comparison\n",
    "        print(\"Average SQL Complexity Metrics:\")\n",
    "        metrics = ['length', 'joins', 'conditions', 'aggregations']\n",
    "        for metric in metrics:\n",
    "            print(f\"{metric.capitalize()}: English={en_avg[metric]:.2f}, Farsi={fa_avg[metric]:.2f}\")\n",
    "        \n",
    "        # Plot complexity comparison\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, [en_avg[m] for m in metrics], width, label='English')\n",
    "        plt.bar(x + width/2, [fa_avg[m] for m in metrics], width, label='Farsi')\n",
    "        \n",
    "        plt.xlabel('Metrics')\n",
    "        plt.ylabel('Average Value')\n",
    "        plt.title('SQL Complexity Comparison')\n",
    "        plt.xticks(x, [m.capitalize() for m in metrics])\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc9013-7d4d-45cc-be63-25043d20581a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
