{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ed78f7-5dff-46ac-8651-98e35e4471c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import setup\n",
    "import os\n",
    "import numpy as np\n",
    "from decouple import config, AutoConfig\n",
    "config = AutoConfig(search_path=\"/home/harry/chatbotDjango\") \n",
    "\n",
    "setup.init_django()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c86d61c-c2d6-48dc-9591-51d8f155305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffeshop.models import EmployeeRole, Employee, ProductType, Product, InventoryItem, ProductInventoryRequirement\n",
    "from coffeshop import services\n",
    "from llama_index.core.schema import TextNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8b8dbf-928a-4446-b340-a6e2828f3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe2d406-c30a-4481-99d9-767b4923be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "MISTRAL_API_KEY = config(\"MISTRAL_API_KEY\")\n",
    "\n",
    "EMEDDING_LENGTH=config(\"EMEDDING_LENGTH\", default=1024, cast=int)\n",
    "\n",
    "LLM_CONFIG = {\n",
    "    \"api_key\" : MISTRAL_API_KEY,\n",
    "}\n",
    "\n",
    "EMBED_CONFIG = {\n",
    "    \"api_key\" : MISTRAL_API_KEY,\n",
    "    \"model_name\" : \"mistral-embed\"\n",
    "}\n",
    "\n",
    "llm = MistralAI(**LLM_CONFIG)\n",
    "embed_model = MistralAIEmbedding(**EMBED_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394bcf4c-d57a-413c-9ca4-a73c61cf1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba60056-2d72-4d9d-9430-a78a0506ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    EmployeeRole,\n",
    "    Employee,\n",
    "    ProductType,\n",
    "    Product,\n",
    "    InventoryItem,\n",
    "    ProductInventoryRequirement\n",
    "]\n",
    "\n",
    "table_names = [model.__name__.lower() + \"s\" for model in models]\n",
    "vector_db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52327879-03f3-4f3d-ab7e-db715c96200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config, AutoConfig\n",
    "config = AutoConfig(search_path=\"/home/harry/chatbotDjango\") \n",
    "DATABASE_URL = config(\"DATABASE_URL_POOL\")\n",
    "if DATABASE_URL.startswith(\"postgres://\"):\n",
    "    DATABASE_URL = DATABASE_URL.replace(\"postgres://\", \"postgresql://\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c964a5a-5cda-4799-885a-b5260eb18984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(DATABASE_URL, isolation_level=\"AUTOCOMMIT\")\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT 1 FROM pg_database WHERE datname = :db_name\"), {\"db_name\": vector_db_name})\n",
    "    db_exists = result.scalar() == 1\n",
    "    \n",
    "    if not db_exists:\n",
    "        connection.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector WITH SCHEMA public\"))\n",
    "        connection.execute(text(f\"CREATE DATABASE {vector_db_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef8361d-e4a0-4ac1-af7b-18c2f272d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "url = make_url(DATABASE_URL)\n",
    "vector_stores = {}\n",
    "\n",
    "for model, table_name in zip(models, table_names):\n",
    "    vector_store = PGVectorStore.from_params(\n",
    "        database=vector_db_name,\n",
    "        host=url.host,\n",
    "        password=url.password,\n",
    "        port=url.port or 5432,\n",
    "        user=url.username,\n",
    "        table_name=table_name,\n",
    "        embed_dim=EMEDDING_LENGTH,\n",
    "    )\n",
    "    vector_stores[model.__name__] = vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df0541da-cf28-47a2-89bd-98391753e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "indices = {}\n",
    "query_engines = {}\n",
    "\n",
    "for model, table_name in zip(models, table_names):\n",
    "    instances = model.objects.all()\n",
    "    \n",
    "    nodes = [\n",
    "        TextNode(\n",
    "            text=str(instance),\n",
    "            metadata={\n",
    "                \"model\": model.__name__,\n",
    "                \"id\": instance.id,\n",
    "                \"created_at\": instance.created_at.isoformat() if hasattr(instance, 'created_at') else None\n",
    "            }\n",
    "        ) for instance in instances\n",
    "    ]\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=vector_stores[model.__name__]\n",
    "    )\n",
    "    indices[model.__name__] = VectorStoreIndex(\n",
    "        nodes, storage_context=storage_context\n",
    "    )\n",
    "    query_engines[model.__name__] = indices[model.__name__].as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a11f3653-0072-4915-bece-b773545fab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from datetime import datetime\n",
    "\n",
    "def should_include_in_index(obj) -> bool:\n",
    "    \"\"\"Filter logic for indexing\"\"\"\n",
    "    if hasattr(obj, 'is_active') and not obj.is_active:\n",
    "        return False\n",
    "    if hasattr(obj, 'is_available') and not obj.is_available:\n",
    "        return False\n",
    "    return True\n",
    "RELEVANT_MODELS = [Employee, ProductType, Product, InventoryItem]\n",
    "\n",
    "docs = []\n",
    "for model in RELEVANT_MODELS:\n",
    "    qs = model.objects.all()\n",
    "    for obj in qs:\n",
    "        if not should_include_in_index(obj):\n",
    "            continue\n",
    "            \n",
    "        docs.append(Document(\n",
    "            text=obj.get_embedding_text_raw(),\n",
    "            doc_id=f\"{model.__name__.lower()}_{obj.id}\",\n",
    "            metadata={\n",
    "                \"model_type\": model.__name__,\n",
    "                \"pk\": obj.pk,\n",
    "                \"last_updated\": datetime.now().isoformat(),\n",
    "                **{\n",
    "                    field.name: getattr(obj, field.name)\n",
    "                    for field in model._meta.fields\n",
    "                    if field.name in ['name', 'description', 'hire_date', 'price']\n",
    "                }\n",
    "            }\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e278d9ab-30f0-4088-81e7-1b86c1cddd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "def handle_document_update(doc: Document):\n",
    "    model_name = doc.metadata[\"model_type\"]\n",
    "    index = indices.get(model_name)\n",
    "    vector_store = vector_stores.get(model_name)\n",
    "    \n",
    "    if not index or not vector_store:\n",
    "        print(f\"Skipping document for unconfigured model: {model_name}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with StorageContext.from_defaults(vector_store=vector_store) as storage_context:\n",
    "            index.delete_ref_doc(doc.doc_id)\n",
    "            index.insert(doc, storage_context=storage_context)\n",
    "            model = globals()[model_name]\n",
    "            obj = model.objects.get(pk=doc.metadata[\"pk\"])\n",
    "            obj.last_indexed = datetime.now()\n",
    "            obj.save(update_fields=['last_indexed'])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed processing {doc.doc_id}: {str(e)}\")\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    executor.map(handle_document_update, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77c0a930-06dd-4e99-90f4-8a699d7991ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Roles Response: Empty Response\n",
      "Products Response: Empty Response\n"
     ]
    }
   ],
   "source": [
    "employee_role_response = query_engines[\"EmployeeRole\"].query(\n",
    "    \"List all employee roles and their permissions\"\n",
    ")\n",
    "print(f\"Employee Roles Response: {employee_role_response}\")\n",
    "\n",
    "product_response = query_engines[\"Product\"].query(\n",
    "    \"What are the top 5 most expensive products?\"\n",
    ")\n",
    "print(f\"Products Response: {product_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98383b29-03dd-4354-b17a-aefe66ac5dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
